[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Runner, PhD Student, and baseball fan - but not necessarily in that order.\n  \n  \n  \n  Resume\n  \n\n  Resume\n\n\n  Elias Oliver Chang\n  Santa Clara, CA\n  elochang (at) ucsc.edu |\n  linkedin.com/in/oliver-chang-423a10171 |\n  github.com/oliverc1623\n\n\nEducation\n\n\n\n\n\nUniversity of California Santa Cruz\nSanta Cruz, CA\n\n\nPh.D. & M.S. Computer Science 3.90/4.00 GPA\n09/2022 – 06/2027\n\n\n\n  Relevant courses: AI, Deep Learning (TensorFlow/PyTorch), Computer Architecture, Compiler Design, NLP (Transformers)\n\n\n\n\n\nPomona College\nClaremont, CA\n\n\nB.A. Computer Science & Mathematics 3.62/4.00 GPA\n09/2018 – 05/2022\n\n\n\n  Thesis: “Fast Matrix Multiplication: A Song of Mathematics and Computer Science”\n\n\n\nExperience\n\n\n\n\n\nGraduate Student Researcher\nSanta Cruz, CA\n\n\nUniversity of California Santa Cruz\n09/2022 – Present\n\n\n\n  \n    Invented transferable RL algorithm for continuous control (PyTorch + SAC), achieving 67% higher reward than baseline.\n    Vectorized algorithm with Torch Compile & CUDA Graphs, 5× faster across 500k timesteps.\n    Developed a novel SAC reward function by using an MTL-robustness formulation to train an adversarial RL agent, successfully finding vulnerabilities in a 2 dimensional search space instead of 1 dimension.\n    First‑author paper in preparation for AAAI 2026.\n  \n\n\n\n\n\nAI Program Coordinator Intern\nSan Jose, CA\n\n\nLumentum\n06/2024 – 09/2024\n\n\n\n  \n    Automated company-wide meeting summaries by developing an Azure Function app that retrieves Teams meeting transcripts via Microsoft Graph API, saving the company 90 hours of manual labor.\n    Saved $250,000/year in liscensing fees by implementing summarizer with OpenAI API (GPT-4o) and Azure Function, eliminating the need for Microsoft Teams Premium for 4,500 employees.\n    Wrote Python code to Prompt Engineer consistent summary generation, generate markdown, and distribute email meeting organizers, resulting in a bitbucket codebase that is used and maintained among 2 employee engineers.\n    Improved model training workflows by implementing data integrity checks and handling compatibility with DuckDB, AWS S3, and PyTests, contributing to a CNN (EfficientNetV2) model achieving an AUC of 0.97.\n  \n\n\n\n\n\nResearch Assistant\nClaremont, CA\n\n\nPomona College\n05/2021 – 07/2022\n\n\n\n  \n    Boosted CNN domain adaptation by 7% via visual perturbation dataset generation.\n    Demonstrated RNN ≥ ResNet by 9% through Seaborn visualisation and statistical tests.\n    First‑author paper, IEEE SSCI 2021.\n  \n\n\n\nProjects\n\n\n\n\n\nCollaborative Embodied Reasoning in AVs [code]\n09/2023 – 08/2024\n\n\n\n  \n    Improved CARLA decision‑making using ChatGPT, LLaMA, and LLaVA; +50 test passes.\n    Curated 20k‑image dataset for LLaVA fine‑tuning, yielding concise scene descriptions.\n    Docker + Kubernetes deployment used by six lab members for remote GUI runs.\n  \n\n\n\n\n\nDeep Learning Course Project [code]\n09/2023 – 12/2023\n\n\n\n  \n    Achieved 72% accuracy on composite dataset via ResNet‑152 fine‑tuning (PyTorch).\n    +15% improvement using cosine LR schedule, weighted sampling & image augments.\n    Top‑quartile performance in class Kaggle competition (30 students).\n  \n\n\n\nPublications\n\nChang O., Kamat A.A., Self W. “Collaborative Embodied Reasoning in Autonomous Driving”. Training Agents with Foundation Models, RLC 2024.\nChang O. “Solving Phase Ordering with Off‑Policy DRL”. EuroLLVM 2024.\nChang O., Marchese C. et al. “Investigating NN Architectures for Autonomous Navigation”. IEEE SSCI 2021.\nChang O., Gilpin L. “Policy Gradient for Image‑Based AVs”. BayLearn 2023.\nRoberto C., Chang O. et al. “eXplainable AI for DRL”. AAAI 2024."
  },
  {
    "objectID": "posts/dodgers-may25/index.html",
    "href": "posts/dodgers-may25/index.html",
    "title": "State of the Dodgers: Dominant Start or Warning Signs Under the Surface? (Mid May 2025 Check-in)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nfrom helper import process_baseball_data\nimport plotly.graph_objects as go\n\nREFERENCE_YEAR = 2025\n\n\nThe Dodgers, defending World Series champs, entered 2025 with sky-high expectations. How are they measuring up about 33 games in?\nAs of early May, they boast a stellar 27-14 (.659) record, sitting atop the NL West with a 1-game lead over the Padres. They’re recently split a 4-game series against the Arizona Diamondbacks. The Dbacks could very well be a play-off opponent. A series split, coupled with a sub-0.500 record against winning teams, could orchestrate a rivertting play-off scene.\n\nNL West Standings\n\n\nTeam\nW\nL\nPCT\nL10\n\n\n\n\nDodgers\n27\n14\n0.659\n6-4\n\n\nPadres\n25\n14\n0.641\n7-3\n\n\nGiants\n24\n18\n0.571\n5-5\n\n\nDiamondbacks\n22\n20\n0.524\n5-5\n\n\nRockies\n7\n34\n0.171\n1-9\n\n\n\n\nOffense Comparison\nThey surpass their record from one year ago today, where they were 27-14 and 41 games into the season. Let’s see how their expected wOBA fares from last year.\n\n\nCode\n# data load and prep\nxwoba_2025 = \"data/team-xwoba-2025.csv\"\nxwoba_2024 = \"data/team-xwoba-2024.csv\"\ndf_2025 = process_baseball_data(xwoba_2025, 2025, end_date_str=\"2025-05-03\")\ndf_2024 = process_baseball_data(xwoba_2024, 2024, end_date_str=\"2024-05-03\")\n\ndf = pd.concat([df_2025, df_2024], ignore_index=True)\ndf[\"Plot Date\"] = df['Game Date'].apply(\n    lambda dt: pd.Timestamp(year=REFERENCE_YEAR, month=dt.month, day=dt.day)\n)\ndf[\"Year\"] = df[\"Game Date\"].dt.year\ndf = df.sort_values(by=\"Plot Date\")\n\nfig = px.line(df, x=\"Plot Date\", y=\"Rolling XWOBA\", color=\"Year\", markers=True,\n    color_discrete_sequence=['blue', 'orange'])\n\nleague_average_xwoba_2025 = 0.326\nfig.add_hline(y=league_average_xwoba_2025,\n    line_dash=\"dash\",\n    line_color=\"red\",\n    annotation_text=f\"2025 MLB Avg XWOBA ({league_average_xwoba_2025})\",\n    annotation_position=\"bottom right\")\n\n# Update layout and legend\nfig.update_layout(\n    xaxis_title='Game Date',\n    yaxis_title='Rolling XWOBA',\n    hovermode='x unified',\n    title=\"Dodgers xwOBA (2024 vs 2025)\"\n)\n\nfig.show()\n\n\n\n\n\n\n\n        \n        \n        \n\n\n(a) Team XWOBA\n\n\n\n\n\n\n                            \n                                            \n\n\n(b)\n\n\n\n\n\nFigure 1\n\n\n\n\nA hot start for the Dodgers! Figure 1 illustrates the Dodgers’ team rolling xwOBA from the start of the season throughout mid May for both 2024 and 2025.\nWe can see that the 2025 Dodgers started the season significantly hotter offensively compared to 2024, based on expected wOBA. However, their performance has dipped recently, bringing their rolling xwOBA closer to their 2024 levels around the same point in the season. Both years show the Dodgers performing well above the 2025 league average xwOBA.\nWill the rest of 2025 look like 2024?\n\n\nCode\n# data load and prep\nxwoba_2025 = \"data/team-xwoba-2025.csv\"\nxwoba_2024 = \"data/team-xwoba-2024.csv\"\ndf_2025 = process_baseball_data(xwoba_2025, 2025, end_date_str=\"2025-05-03\")\ndf_2024 = process_baseball_data(xwoba_2024, 2024, end_date_str=\"2024-10-01\")\n\ndf = pd.concat([df_2025, df_2024], ignore_index=True)\ndf[\"Plot Date\"] = df['Game Date'].apply(\n    lambda dt: pd.Timestamp(year=REFERENCE_YEAR, month=dt.month, day=dt.day)\n)\ndf[\"Year\"] = df[\"Game Date\"].dt.year\ndf = df.sort_values(by=\"Plot Date\")\n\nfig = px.line(df, x=\"Plot Date\", y=\"Rolling XWOBA\", color=\"Year\", markers=True,\n    color_discrete_sequence=['blue', 'orange'])\n\nleague_average_xwoba_2024 = 0.312\nfig.add_hline(y=league_average_xwoba_2024,\n    line_dash=\"dash\",\n    line_color=\"red\",\n    annotation_text=f\"2024 MLB Avg XWOBA ({league_average_xwoba_2024})\",\n    annotation_position=\"bottom right\")\n\nfig.update_layout(\n    xaxis_title='Game Date',\n    yaxis_title='Rolling XWOBA',\n    hovermode='x unified',\n    title=\"Dodgers xwOBA (2024 vs 2025)\"\n)\n\nfig.show()\n\n\n\n\n                            \n                                            \n\n\nFigure 2: Team XWOBA 2024\n\n\n\n\nLet’s sure hope we do not witness another late-May struggle.\n\n\nShohei Ohtani\nWhile the team’s overall offensive performance has shown some regression towards 2024 levels, it’s important to look at individual player contributions. Let’s examine Shohei Ohtani’s rolling xwOBA as an example.\n\nI am hedging my bets that Ohtani will win NL MVP again. Observe that Ohtani exhibits cylical patterns in his rolling xwOBA.\n\nIn 2021, Ohtani brandished an above 0.500 xwOBA during the summer months of the season. In 2021, writes a similar story to 2021 with the addition of a 1.0 OPS in August. Last year, a year unlike any other, Ohtani wins MVP, concluding his 2024 campaign with a 1.225 OPS in September and October.\nAre we due for another Shohei Summer?\nI rooting for it. However, with Ohtani pitching on the horizon after the all-star break, it will interesting to see how that will influence with offensive performance heading into the fall. 2023 Ohtani was as close as we will ever get to the most athletically impressive season in all of baseball. His batting slash line was .304/.412/.654 and boasted a 142 ERA+ on the season. Unfortunately, Ohtani had a season ending injury in his throwing elbow in 2023.\nIs this symbolic of Icarsus’s Hubris? 2025, 2026, and 2027 will determine if Ohtani is Icarsus or a living God, standing among mere mortals. With only half a season of pitching, it is unkliekly Ohtani will garner any Cy Young votes. However, a full season of pitching, alongside a 1.0 OPS, will cement Ohtani as the greatest of all time."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Running on Numbers",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLucky and Unlucky Hitters\n\n\n\npython\n\nanalysis\n\nMLB\n\n\n\n\n\n\n\n\n\nMay 23, 2025\n\n\nOliver Chang\n\n\n\n\n\n\n\n\n\n\n\n\nState of the Dodgers: Dominant Start or Warning Signs Under the Surface? (Mid May 2025 Check-in)\n\n\n\npython\n\nanalysis\n\nDodgers\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nOliver Chang\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Papers/Projects",
    "section": "",
    "text": "XWOBA VS WOBA\n\n\n\ncode\n\nanalysis\n\nDodgers\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nOliver Chang\n\n\n\n\n\n\n\n\n\n\n\n\nState of the Dodgers: Dominant Start or Warning Signs Under the Surface? (Mid May 2025 Check-in)\n\n\n\ncode\n\nanalysis\n\nDodgers\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nOliver Chang\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 3, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 30, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers/dodgers-may25/index.html",
    "href": "papers/dodgers-may25/index.html",
    "title": "State of the Dodgers: Dominant Start or Warning Signs Under the Surface? (Mid May 2025 Check-in)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nfrom helper import process_baseball_data\nimport plotly.graph_objects as go\n\nREFERENCE_YEAR = 2025\n\n\nThe Dodgers, defending World Series champs, entered 2025 with sky-high expectations. How are they measuring up about 33 games in?\nAs of early May, they boast a stellar 27-14 (.659) record, sitting atop the NL West with a 1-game lead over the Padres. They’re recently split a 4-game series against the Arizona Diamondbacks. The Dbacks could very well be a play-off opponent. A series split, coupled with a sub-0.500 record against winning teams, could orchestrate a rivertting play-off scene.\n\nNL West Standings\n\n\nTeam\nW\nL\nPCT\nL10\n\n\n\n\nDodgers\n27\n14\n0.659\n6-4\n\n\nPadres\n25\n14\n0.641\n7-3\n\n\nGiants\n24\n18\n0.571\n5-5\n\n\nDiamondbacks\n22\n20\n0.524\n5-5\n\n\nRockies\n7\n34\n0.171\n1-9\n\n\n\n\nOffense Comparison\nThey surpass their record from one year ago today, where they were 27-14 and 41 games into the season. Let’s see how their expected wOBA fares from last year.\n\n\nCode\n# data load and prep\nxwoba_2025 = \"data/team-xwoba-2025.csv\"\nxwoba_2024 = \"data/team-xwoba-2024.csv\"\ndf_2025 = process_baseball_data(xwoba_2025, 2025, end_date_str=\"2025-05-03\")\ndf_2024 = process_baseball_data(xwoba_2024, 2024, end_date_str=\"2024-05-03\")\n\ndf = pd.concat([df_2025, df_2024], ignore_index=True)\ndf[\"Plot Date\"] = df['Game Date'].apply(\n    lambda dt: pd.Timestamp(year=REFERENCE_YEAR, month=dt.month, day=dt.day)\n)\ndf[\"Year\"] = df[\"Game Date\"].dt.year\ndf = df.sort_values(by=\"Plot Date\")\n\nfig = px.line(df, x=\"Plot Date\", y=\"Rolling XWOBA\", color=\"Year\", markers=True,\n    color_discrete_sequence=['blue', 'orange'])\n\nleague_average_xwoba_2025 = 0.326\nfig.add_hline(y=league_average_xwoba_2025,\n    line_dash=\"dash\",\n    line_color=\"red\",\n    annotation_text=f\"2025 MLB Avg XWOBA ({league_average_xwoba_2025})\",\n    annotation_position=\"bottom right\")\n\n# Update layout and legend\nfig.update_layout(\n    xaxis_title='Game Date',\n    yaxis_title='Rolling XWOBA',\n    hovermode='x unified',\n    title=\"Dodgers xwOBA (2024 vs 2025)\"\n)\n\nfig.show()\n\n\n\n\n\n\n\n        \n        \n        \n\n\n(a) Team XWOBA\n\n\n\n\n\n\n                            \n                                            \n\n\n(b)\n\n\n\n\n\nFigure 1\n\n\n\n\nA hot start for the Dodgers! Figure 1 illustrates the Dodgers’ team rolling xwOBA from the start of the season throughout mid May for both 2024 and 2025.\nWe can see that the 2025 Dodgers started the season significantly hotter offensively compared to 2024, based on expected wOBA. However, their performance has dipped recently, bringing their rolling xwOBA closer to their 2024 levels around the same point in the season. Both years show the Dodgers performing well above the 2025 league average xwOBA.\nWill the rest of 2025 look like 2024?\n\n\nCode\n# data load and prep\nxwoba_2025 = \"data/team-xwoba-2025.csv\"\nxwoba_2024 = \"data/team-xwoba-2024.csv\"\ndf_2025 = process_baseball_data(xwoba_2025, 2025, end_date_str=\"2025-05-03\")\ndf_2024 = process_baseball_data(xwoba_2024, 2024, end_date_str=\"2024-10-01\")\n\ndf = pd.concat([df_2025, df_2024], ignore_index=True)\ndf[\"Plot Date\"] = df['Game Date'].apply(\n    lambda dt: pd.Timestamp(year=REFERENCE_YEAR, month=dt.month, day=dt.day)\n)\ndf[\"Year\"] = df[\"Game Date\"].dt.year\ndf = df.sort_values(by=\"Plot Date\")\n\nfig = px.line(df, x=\"Plot Date\", y=\"Rolling XWOBA\", color=\"Year\", markers=True,\n    color_discrete_sequence=['blue', 'orange'])\n\nleague_average_xwoba_2024 = 0.312\nfig.add_hline(y=league_average_xwoba_2024,\n    line_dash=\"dash\",\n    line_color=\"red\",\n    annotation_text=f\"2024 MLB Avg XWOBA ({league_average_xwoba_2024})\",\n    annotation_position=\"bottom right\")\n\nfig.update_layout(\n    xaxis_title='Game Date',\n    yaxis_title='Rolling XWOBA',\n    hovermode='x unified',\n    title=\"Dodgers xwOBA (2024 vs 2025)\"\n)\n\nfig.show()\n\n\n\n\n                            \n                                            \n\n\nFigure 2: Team XWOBA 2024\n\n\n\n\nLet’s sure hope we do not witness another late-May struggle.\n\n\nShohei Ohtani\nWhile the team’s overall offensive performance has shown some regression towards 2024 levels, it’s important to look at individual player contributions. Let’s examine Shohei Ohtani’s rolling xwOBA as an example.\n\nI am hedging my bets that Ohtani will win NL MVP again. Observe that Ohtani exhibits cylical patterns in his rolling xwOBA.\n\nIn 2021, Ohtani brandished an above 0.500 xwOBA during the summer months of the season. In 2021, writes a similar story to 2021 with the addition of a 1.0 OPS in August. Last year, a year unlike any other, Ohtani wins MVP, concluding his 2024 campaign with a 1.225 OPS in September and October.\nAre we due for another Shohei Summer?\nI rooting for it. However, with Ohtani pitching on the horizon after the all-star break, it will interesting to see how that will influence with offensive performance heading into the fall. 2023 Ohtani was as close as we will ever get to the most athletically impressive season in all of baseball. His batting slash line was .304/.412/.654 and boasted a 142 ERA+ on the season. Unfortunately, Ohtani had a season ending injury in his throwing elbow in 2023.\nIs this symbolic of Icarsus’s Hubris? 2025, 2026, and 2027 will determine if Ohtani is Icarsus or a living God, standing among mere mortals. With only half a season of pitching, it is unkliekly Ohtani will garner any Cy Young votes. However, a full season of pitching, alongside a 1.0 OPS, will cement Ohtani as the greatest of all time."
  },
  {
    "objectID": "papers/post-with-code/index.html",
    "href": "papers/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "papers/welcome/index.html",
    "href": "papers/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/xwoba_regression/index.html",
    "href": "posts/xwoba_regression/index.html",
    "title": "Lucky and Unlucky Hitters",
    "section": "",
    "text": "There is no denying that baseball has an element of luck. Sometimes the hardest hit balls find a glove, and sometimes a blooper falls in for a hit. We can try to quantify this luck by looking at the difference between a player’s actual stats and their expected stats.\n\nExpected Batting Average (xBA)\nFirst let’s take a look at expected batting average (xBA). We grab the latest expected statistics leaderboard from baseball savant.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"expected_stats.csv\")\ndf[\"est_ba_minus_ba_diff\"].describe().round(4)\n\n\ncount    251.0000\nmean      -0.0069\nstd        0.0283\nmin       -0.0750\n25%       -0.0260\n50%       -0.0060\n75%        0.0125\nmax        0.0660\nName: est_ba_minus_ba_diff, dtype: float64\n\n\nThe unluckiest batter has as a difference of -0.075, while the luckiest batter has a difference of 0.066. As it stands, Joc Pederson of the Texas Ranges holds unluckiest BA and Pavin Smith of the Arizona Diamondbacks boasts the luckiest BA. Interestingly, the mean xBA is -0.0069, suggesting that the average MLB hitter (with the minimum number of BIP/PA) is slightly unlucky when it comes to batting average.\n\n\nAbove shows Joc Pederson lining out to Samad Taylor. The batted ball had a hit probability of 78% per baseballsavant. Conversely, Pavin Smith’s single to Hyeseong Kim had a 94% chance of becoming an out.\n\n\nPederson and Smith are just two players on difference ends of the spectrum. Here is a histogram, illustrating the disitrbution of expected batting average differences. Oberve the slight left-skew distribution.\n\n\nCode\nimport plotly.express as px\nfig =  px.histogram(df, x=\"est_ba_minus_ba_diff\", marginal=\"rug\", hover_data=[\"last_name, first_name\"],\n    labels = {\n        \"est_ba_minus_ba_diff\": \"BA - xBA\",\n        \"count\": \"Count\"\n    },\n    title=\"Distribution of xBA Difference\"\n)\nfig.show()\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nPretty close correlation between xBA and BA with an \\(R^2\\) value of 0.48. xBA is not a perfect indicator of BA. In fact, xBA should be interpreted more heavily than BA given that it removes luck from the at-bat. Note that there remains a sizable discrepancy between xBA and BA, suggesting a sizable “luck” component observed in BA that xBA does not capture.\n\n\nCode\nfrom sklearn.metrics import r2_score\n\nfig = px.scatter(df, x=\"est_ba\", y=\"ba\", hover_data=[\"last_name, first_name\"], trendline=\"ols\",\n    labels = {\n        \"est_ba\": \"xBA\",\n        \"ba\": \"BA\"\n    },\n    title=\"Relationship Between xBA and BA\"\n)\n\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nExpected Weighted OBA (xwOBA)\nxwOBA is what statcast considers, the most important offensive metric that tells “the story of a player’s season based on the quality of and amount of contact, not coutomes.” In sum, xwOBA weighs hit types and walks with an expected run value based on historically similar batted ball events. The formulation of the coefficients use exit velocity, launch angle, and sprint speed (for topped or weakly hit balls).\n\n\nCode\ndf[\"est_woba_minus_woba_diff\"].describe().round(4)\n\n\ncount    251.0000\nmean      -0.0107\nstd        0.0313\nmin       -0.0940\n25%       -0.0300\n50%       -0.0110\n75%        0.0130\nmax        0.0580\nName: est_woba_minus_woba_diff, dtype: float64\n\n\nThe mean wOBA-xwOBA difference is -0.0107. This is surpsignly greater than the mean difference of BA and xBA. In addition, the most unlucky batter with xwOBA is most unlucky than the most unlucky batter with xBA. Simiarly, the most lucky xwOBA batter is not as lucky as the most lucky xBA batter. Overall it seems like player are less lucky with xwOBA. A reason for this discrepency is that xwOBA weighs batted ball events with difference values. For example, a double is weighted more heavily than a single. As such a batted ball who’s exit velocity and launch angle that would likely result in a double will increase the diffence in xwOBA and wOBA if that double becomes a single.\n\n\nCode\nfig =  px.histogram(df, x=\"est_woba_minus_woba_diff\", marginal=\"rug\", hover_data=[\"last_name, first_name\"], \n        labels = {\n        \"diff\": \"wOBA - xwOBA\",\n        \"count\": \"Count\"\n    },\n    title=\"Distribution of wOBA-xwOBA\")\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nCombining Differences\n\n\nCode\nid_cols = ['last_name, first_name', 'player_id', 'pa', 'year']\n\n# melt the three diff-cols into long form\ndf_long = df.melt(\n    id_vars=id_cols,\n    value_vars=[\n        'est_ba_minus_ba_diff',\n        'est_slg_minus_slg_diff',\n        'est_woba_minus_woba_diff'\n    ],\n    var_name='metric',\n    value_name='diff'\n)\n# map the verbose column names to simple metric labels\ndf_long['metric'] = df_long['metric'].map({\n    'est_ba_minus_ba_diff':   'BA',\n    'est_slg_minus_slg_diff': 'SLG',\n    'est_woba_minus_woba_diff':'wOBA'\n})\n\nfig = px.histogram(df_long, x=\"diff\", color=\"metric\", marginal=\"rug\", hover_data=[\"last_name, first_name\"], \n        labels = {\n        \"diff\": \"Metric - Expected Metric\",\n        \"count\": \"Count\"\n    },\n    title=\"Distribution of xBA, xSLG, and xwOBA Differences\"\n)\nfig.show()\n\n\n                            \n                                            \n\n\nObserve the histogram above that combines xBA, expected slugging (xSLG), and xwOBA. Among xBA, xSLG, and xwOBA, xSLG shows the greated spread in difference values. This is likely a result of how SLG assign weights to hits. Specifically, SLG is defined as (1B + 2Bx2 + 3Bx3 + HRx4)/AB. Unlike, wOBA, slugging weights are fixed and higher-valued. Hence, a batted ball that does not result in its expected outcome faces a greater difference penalty.\nWho’s that unlucky slugger to the very left? That’s Royals veteran Salvador Perez.\n\n\nThat ball had a 97% hit probability and would have been a homerun in 9/30 ballparks.\nSalvador Pérez’s recent flyout underscores the value of integrating ballpark- and weather-specific factors into our expected-stat models. Although current metrics leverage historical launch-angle and exit-velocity data to predict outcomes, they omit critical dimensions—namely batted-ball trajectory, precise horizontal launch direction, and wind conditions. In Pérez’s case, a strong tailwind or a gust toward the wall might readily have turned that routine out into a home run. That said, Statcast remains an extraordinary tool for quantifying the game, and enriching it with environmental and park-design variables promises even deeper insights into batted-ball performance.\nThanks for reading!"
  },
  {
    "objectID": "posts/xwoba_regression/scraps.html",
    "href": "posts/xwoba_regression/scraps.html",
    "title": "Running on Numbers",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv(\"expected_stats.csv\")\ndf.head(10)\n\n\n\n\n\n\n\n\nlast_name, first_name\nplayer_id\nyear\npa\nbip\nba\nest_ba\nest_ba_minus_ba_diff\nslg\nest_slg\nest_slg_minus_slg_diff\nwoba\nest_woba\nest_woba_minus_woba_diff\n\n\n\n\n0\nDuran, Jarren\n680776\n2025\n239\n169\n0.268\n0.270\n-0.002\n0.418\n0.405\n0.013\n0.321\n0.325\n-0.004\n\n\n1\nCarroll, Corbin\n682998\n2025\n238\n155\n0.271\n0.284\n-0.013\n0.579\n0.612\n-0.033\n0.393\n0.408\n-0.015\n\n\n2\nDevers, Rafael\n646240\n2025\n236\n140\n0.299\n0.303\n-0.004\n0.557\n0.590\n-0.033\n0.416\n0.431\n-0.015\n\n\n3\nTucker, Kyle\n663656\n2025\n235\n172\n0.276\n0.306\n-0.030\n0.543\n0.583\n-0.040\n0.392\n0.415\n-0.023\n\n\n4\nOhtani, Shohei\n660271\n2025\n232\n146\n0.302\n0.335\n-0.033\n0.643\n0.751\n-0.108\n0.433\n0.481\n-0.048\n\n\n5\nLindor, Francisco\n596019\n2025\n231\n168\n0.277\n0.272\n0.005\n0.470\n0.467\n0.003\n0.356\n0.354\n0.002\n\n\n6\nBichette, Bo\n666182\n2025\n231\n176\n0.284\n0.308\n-0.024\n0.409\n0.486\n-0.077\n0.324\n0.362\n-0.038\n\n\n7\nChourio, Jackson\n694192\n2025\n229\n171\n0.250\n0.246\n0.004\n0.423\n0.393\n0.030\n0.302\n0.287\n0.015\n\n\n8\nNootbaar, Lars\n663457\n2025\n229\n159\n0.247\n0.270\n-0.023\n0.407\n0.462\n-0.055\n0.336\n0.362\n-0.026\n\n\n9\nHarper, Bryce\n547180\n2025\n229\n154\n0.271\n0.288\n-0.017\n0.464\n0.514\n-0.050\n0.369\n0.391\n-0.022"
  },
  {
    "objectID": "posts/xwoba_regression/index.html#statistic",
    "href": "posts/xwoba_regression/index.html#statistic",
    "title": "Lucky and Unlucky Hitters",
    "section": "statistic",
    "text": "statistic\n\ne s t  b a  m i n u s  b a  d i f f | | - - - - - - - - - - - - - - - - - - - - - - - : | | 2 5 1 | | - 0 . 0 0 6 9 | | 0 . 0 2 8 3 | | - 0 . 0 7 5 | | - 0 . 0 2 6 | | - 0 . 0 0 6 | | 0 . 0 1 2 5 | | 0 . 0 6 6 |"
  },
  {
    "objectID": "posts/predicting-xwoba-with-swingpath/index.html",
    "href": "posts/predicting-xwoba-with-swingpath/index.html",
    "title": "Predicting xwOBA with Swing Path",
    "section": "",
    "text": "Code\nimport pandas as pd\n\ndf = pd.read_csv(\"data.csv\")\ndf.describe().round(4)\n\n\n\n\n\n\n\n\n\nplayer_id\nyear\npa\nk_percent\nbb_percent\nwoba\nxwoba\navg_swing_speed\nfast_swing_rate\nblasts_contact\n...\nvertical_swing_path\nexit_velocity_avg\nlaunch_angle_avg\nsweet_spot_percent\nbarrel_batted_rate\nhard_hit_percent\navg_best_speed\navg_hyper_speed\nwhiff_percent\nswing_percent\n\n\n\n\ncount\n167.0000\n167.0\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n...\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n167.0000\n\n\nmean\n650394.7066\n2025.0\n207.0120\n20.9347\n9.1892\n0.3288\n0.3419\n71.8329\n24.8970\n15.8551\n...\n32.3749\n90.2198\n13.6096\n34.9461\n9.8970\n43.4850\n100.9210\n94.8817\n24.0994\n47.1653\n\n\nstd\n50465.1126\n0.0\n21.4992\n5.9374\n3.5388\n0.0455\n0.0395\n2.6993\n18.2156\n4.8849\n...\n3.8661\n2.3164\n4.9914\n4.7045\n4.8237\n8.5194\n2.6066\n1.5820\n6.2312\n5.1874\n\n\nmin\n457705.0000\n2025.0\n168.0000\n2.5000\n2.1000\n0.2150\n0.2410\n62.4000\n0.3000\n1.2000\n...\n24.0000\n84.7000\n-0.7000\n22.2000\n0.0000\n16.2000\n92.9649\n90.4909\n4.6000\n34.4000\n\n\n25%\n632970.0000\n2025.0\n189.5000\n17.1000\n6.4500\n0.2995\n0.3165\n70.4000\n9.5500\n13.2500\n...\n29.5000\n88.8000\n10.1000\n31.8500\n6.5500\n39.1000\n99.6790\n94.1121\n19.9500\n43.5000\n\n\n50%\n666018.0000\n2025.0\n208.0000\n21.4000\n8.6000\n0.3280\n0.3390\n72.0000\n21.4000\n16.2000\n...\n32.6000\n90.4000\n13.6000\n35.0000\n9.6000\n44.7000\n100.9913\n94.9669\n24.4000\n47.4000\n\n\n75%\n679205.5000\n2025.0\n224.0000\n25.0500\n11.4500\n0.3555\n0.3610\n73.7000\n38.0000\n18.6000\n...\n34.6500\n91.5500\n16.6000\n38.3000\n12.9000\n48.5000\n102.4390\n95.7131\n28.3500\n50.5000\n\n\nmax\n808982.0000\n2025.0\n258.0000\n39.3000\n20.0000\n0.5170\n0.4780\n78.9000\n77.5000\n30.8000\n...\n45.5000\n97.1000\n24.2000\n47.9000\n25.5000\n65.1000\n109.1147\n100.0271\n40.3000\n61.8000\n\n\n\n\n8 rows × 28 columns\n\n\n\n\n\nCode\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.svm import SVR\n\nmesh_size = .02\nmargin = 0\n\nX = df[[\"attack_angle\", \"attack_direction\"]]\ny = df[[\"xwoba\"]]\n\nmodel = SVR(C=1.)\nmodel.fit(X, y)\n\n# Create a mesh grid on which we will run our model\nx_min, x_max = X.attack_angle.min() - margin, X.attack_angle.max() + margin\ny_min, y_max = X.attack_direction.min() - margin, X.attack_direction.max() + margin\nxrange = np.arange(x_min, x_max, mesh_size)\nyrange = np.arange(y_min, y_max, mesh_size)\nxx, yy = np.meshgrid(xrange, yrange)\n\n# Run model\npred = model.predict(np.c_[xx.ravel(), yy.ravel()])\npred = pred.reshape(xx.shape)\n\n# Generate the plot\nfig = px.scatter_3d(df, x='attack_angle', y='attack_direction', z='xwoba')\nfig.update_traces(marker=dict(size=5))\nfig.add_traces(go.Surface(x=xrange, y=yrange, z=pred, name='pred_surface'))\nfig.show()"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nInvestigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation\n\n\n\npython\n\nDRL\n\nLLMs\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\nOliver Chang, Anuj Ajay Kamat, William Self\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation\n\n\n\npython\n\nCNN\n\nSimulation\n\n\n\n\n\n\n\n\n\nDec 5, 2021\n\n\nOliver Chang, Christiana Marchese, Jared Mejia, Anthony J. Clark\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers/predicting-xwoba-with-swingpath/index.html",
    "href": "papers/predicting-xwoba-with-swingpath/index.html",
    "title": "Predicting xwOBA with Swing Path",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_csv(\"data.csv\")\ndf.describe()\n\n\n\n\n\n\n\n\nplayer_id\nyear\npa\nk_percent\nbb_percent\nwoba\nxwoba\navg_swing_speed\nfast_swing_rate\nblasts_contact\n...\nvertical_swing_path\nexit_velocity_avg\nlaunch_angle_avg\nsweet_spot_percent\nbarrel_batted_rate\nhard_hit_percent\navg_best_speed\navg_hyper_speed\nwhiff_percent\nswing_percent\n\n\n\n\ncount\n167.000000\n167.0\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n...\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n\n\nmean\n650394.706587\n2025.0\n207.011976\n20.934731\n9.189222\n0.328832\n0.341880\n71.832934\n24.897006\n15.855090\n...\n32.374850\n90.219760\n13.609581\n34.946108\n9.897006\n43.485030\n100.920958\n94.881651\n24.099401\n47.165269\n\n\nstd\n50465.112555\n0.0\n21.499226\n5.937424\n3.538838\n0.045545\n0.039535\n2.699340\n18.215634\n4.884863\n...\n3.866098\n2.316422\n4.991405\n4.704468\n4.823692\n8.519352\n2.606584\n1.581966\n6.231222\n5.187438\n\n\nmin\n457705.000000\n2025.0\n168.000000\n2.500000\n2.100000\n0.215000\n0.241000\n62.400000\n0.300000\n1.200000\n...\n24.000000\n84.700000\n-0.700000\n22.200000\n0.000000\n16.200000\n92.964927\n90.490865\n4.600000\n34.400000\n\n\n25%\n632970.000000\n2025.0\n189.500000\n17.100000\n6.450000\n0.299500\n0.316500\n70.400000\n9.550000\n13.250000\n...\n29.500000\n88.800000\n10.100000\n31.850000\n6.550000\n39.100000\n99.678968\n94.112107\n19.950000\n43.500000\n\n\n50%\n666018.000000\n2025.0\n208.000000\n21.400000\n8.600000\n0.328000\n0.339000\n72.000000\n21.400000\n16.200000\n...\n32.600000\n90.400000\n13.600000\n35.000000\n9.600000\n44.700000\n100.991316\n94.966916\n24.400000\n47.400000\n\n\n75%\n679205.500000\n2025.0\n224.000000\n25.050000\n11.450000\n0.355500\n0.361000\n73.700000\n38.000000\n18.600000\n...\n34.650000\n91.550000\n16.600000\n38.300000\n12.900000\n48.500000\n102.439016\n95.713060\n28.350000\n50.500000\n\n\nmax\n808982.000000\n2025.0\n258.000000\n39.300000\n20.000000\n0.517000\n0.478000\n78.900000\n77.500000\n30.800000\n...\n45.500000\n97.100000\n24.200000\n47.900000\n25.500000\n65.100000\n109.114728\n100.027068\n40.300000\n61.800000\n\n\n\n\n8 rows × 28 columns\n\n\n\n\ndf.columns\n\nIndex(['last_name, first_name', 'player_id', 'year', 'pa', 'k_percent',\n       'bb_percent', 'woba', 'xwoba', 'avg_swing_speed', 'fast_swing_rate',\n       'blasts_contact', 'blasts_swing', 'squared_up_contact',\n       'squared_up_swing', 'avg_swing_length', 'swords', 'attack_angle',\n       'attack_direction', 'ideal_angle_rate', 'vertical_swing_path',\n       'exit_velocity_avg', 'launch_angle_avg', 'sweet_spot_percent',\n       'barrel_batted_rate', 'hard_hit_percent', 'avg_best_speed',\n       'avg_hyper_speed', 'whiff_percent', 'swing_percent'],\n      dtype='object')\n\n\n\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.svm import SVR\n\nmesh_size = .02\nmargin = 0\n\nX = df[[\"attack_angle\", \"attack_direction\"]]\ny = df[[\"xwoba\"]]\n\nmodel = SVR(C=1.)\nmodel.fit(X, y)\n\n# Create a mesh grid on which we will run our model\nx_min, x_max = X.attack_angle.min() - margin, X.attack_angle.max() + margin\ny_min, y_max = X.attack_direction.min() - margin, X.attack_direction.max() + margin\nxrange = np.arange(x_min, x_max, mesh_size)\nyrange = np.arange(y_min, y_max, mesh_size)\nxx, yy = np.meshgrid(xrange, yrange)\n\n# Run model\npred = model.predict(np.c_[xx.ravel(), yy.ravel()])\npred = pred.reshape(xx.shape)\n\n# Generate the plot\nfig = px.scatter_3d(df, x='attack_angle', y='attack_direction', z='xwoba')\nfig.update_traces(marker=dict(size=5))\nfig.add_traces(go.Surface(x=xrange, y=yrange, z=pred, name='pred_surface'))\nfig.show()\n\n/Users/oliverchang/opt/anaconda3/envs/baseball/lib/python3.10/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning:\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n\n/Users/oliverchang/opt/anaconda3/envs/baseball/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning:\n\nX does not have valid feature names, but SVR was fitted with feature names"
  },
  {
    "objectID": "papers/xwoba_regression/index.html",
    "href": "papers/xwoba_regression/index.html",
    "title": "Lucky and Unlucky Hitters",
    "section": "",
    "text": "There is no denying that baseball has an element of luck. Sometimes the hardest hit balls find a glove, and sometimes a blooper falls in for a hit. We can try to quantify this luck by looking at the difference between a player’s actual stats and their expected stats.\n\nExpected Batting Average (xBA)\nFirst let’s take a look at expected batting average (xBA). We grab the latest expected statistics leaderboard from baseball savant.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"expected_stats.csv\")\ndf[\"est_ba_minus_ba_diff\"].describe().round(4)\n\n\ncount    251.0000\nmean      -0.0069\nstd        0.0283\nmin       -0.0750\n25%       -0.0260\n50%       -0.0060\n75%        0.0125\nmax        0.0660\nName: est_ba_minus_ba_diff, dtype: float64\n\n\nThe unluckiest batter has as a difference of -0.075, while the luckiest batter has a difference of 0.066. As it stands, Joc Pederson of the Texas Ranges holds unluckiest BA and Pavin Smith of the Arizona Diamondbacks boasts the luckiest BA. Interestingly, the mean xBA is -0.0069, suggesting that the average MLB hitter (with the minimum number of BIP/PA) is slightly unlucky when it comes to batting average.\n\n\nAbove shows Joc Pederson lining out to Samad Taylor. The batted ball had a hit probability of 78% per baseballsavant. Conversely, Pavin Smith’s single to Hyeseong Kim had a 94% chance of becoming an out.\n\n\nPederson and Smith are just two players on difference ends of the spectrum. Here is a histogram, illustrating the disitrbution of expected batting average differences. Oberve the slight left-skew distribution.\n\n\nCode\nimport plotly.express as px\nfig =  px.histogram(df, x=\"est_ba_minus_ba_diff\", marginal=\"rug\", hover_data=[\"last_name, first_name\"],\n    labels = {\n        \"est_ba_minus_ba_diff\": \"BA - xBA\",\n        \"count\": \"Count\"\n    },\n    title=\"Distribution of xBA Difference\"\n)\nfig.show()\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nPretty close correlation between xBA and BA with an \\(R^2\\) value of 0.48. xBA is not a perfect indicator of BA. In fact, xBA should be interpreted more heavily than BA given that it removes luck from the at-bat. Note that there remains a sizable discrepancy between xBA and BA, suggesting a sizable “luck” component observed in BA that xBA does not capture.\n\n\nCode\nfrom sklearn.metrics import r2_score\n\nfig = px.scatter(df, x=\"est_ba\", y=\"ba\", hover_data=[\"last_name, first_name\"], trendline=\"ols\",\n    labels = {\n        \"est_ba\": \"xBA\",\n        \"ba\": \"BA\"\n    },\n    title=\"Relationship Between xBA and BA\"\n)\n\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nExpected Weighted OBA (xwOBA)\nxwOBA is what statcast considers, the most important offensive metric that tells “the story of a player’s season based on the quality of and amount of contact, not coutomes.” In sum, xwOBA weighs hit types and walks with an expected run value based on historically similar batted ball events. The formulation of the coefficients use exit velocity, launch angle, and sprint speed (for topped or weakly hit balls).\n\n\nCode\ndf[\"est_woba_minus_woba_diff\"].describe().round(4)\n\n\ncount    251.0000\nmean      -0.0107\nstd        0.0313\nmin       -0.0940\n25%       -0.0300\n50%       -0.0110\n75%        0.0130\nmax        0.0580\nName: est_woba_minus_woba_diff, dtype: float64\n\n\nThe mean wOBA-xwOBA difference is -0.0107. This is surpsignly greater than the mean difference of BA and xBA. In addition, the most unlucky batter with xwOBA is most unlucky than the most unlucky batter with xBA. Simiarly, the most lucky xwOBA batter is not as lucky as the most lucky xBA batter. Overall it seems like player are less lucky with xwOBA. A reason for this discrepency is that xwOBA weighs batted ball events with difference values. For example, a double is weighted more heavily than a single. As such a batted ball who’s exit velocity and launch angle that would likely result in a double will increase the diffence in xwOBA and wOBA if that double becomes a single.\n\n\nCode\nfig =  px.histogram(df, x=\"est_woba_minus_woba_diff\", marginal=\"rug\", hover_data=[\"last_name, first_name\"], \n        labels = {\n        \"diff\": \"wOBA - xwOBA\",\n        \"count\": \"Count\"\n    },\n    title=\"Distribution of wOBA-xwOBA\")\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nCombining Differences\n\n\nCode\nid_cols = ['last_name, first_name', 'player_id', 'pa', 'year']\n\n# melt the three diff-cols into long form\ndf_long = df.melt(\n    id_vars=id_cols,\n    value_vars=[\n        'est_ba_minus_ba_diff',\n        'est_slg_minus_slg_diff',\n        'est_woba_minus_woba_diff'\n    ],\n    var_name='metric',\n    value_name='diff'\n)\n# map the verbose column names to simple metric labels\ndf_long['metric'] = df_long['metric'].map({\n    'est_ba_minus_ba_diff':   'BA',\n    'est_slg_minus_slg_diff': 'SLG',\n    'est_woba_minus_woba_diff':'wOBA'\n})\n\nfig = px.histogram(df_long, x=\"diff\", color=\"metric\", marginal=\"rug\", hover_data=[\"last_name, first_name\"], \n        labels = {\n        \"diff\": \"Metric - Expected Metric\",\n        \"count\": \"Count\"\n    },\n    title=\"Distribution of xBA, xSLG, and xwOBA Differences\"\n)\nfig.show()\n\n\n                            \n                                            \n\n\nObserve the histogram above that combines xBA, expected slugging (xSLG), and xwOBA. Among xBA, xSLG, and xwOBA, xSLG shows the greated spread in difference values. This is likely a result of how SLG assign weights to hits. Specifically, SLG is defined as (1B + 2Bx2 + 3Bx3 + HRx4)/AB. Unlike, wOBA, slugging weights are fixed and higher-valued. Hence, a batted ball that does not result in its expected outcome faces a greater difference penalty.\nWho’s that unlucky slugger to the very left? That’s Royals veteran Salvador Perez.\n\n\nThat ball had a 97% hit probability and would have been a homerun in 9/30 ballparks.\nSalvador Pérez’s recent flyout underscores the value of integrating ballpark- and weather-specific factors into our expected-stat models. Although current metrics leverage historical launch-angle and exit-velocity data to predict outcomes, they omit critical dimensions—namely batted-ball trajectory, precise horizontal launch direction, and wind conditions. In Pérez’s case, a strong tailwind or a gust toward the wall might readily have turned that routine out into a home run. That said, Statcast remains an extraordinary tool for quantifying the game, and enriching it with environmental and park-design variables promises even deeper insights into batted-ball performance.\nThanks for reading!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nUniversity of California Santa Cruz Santa Cruz, CA Ph.D. & M.S. Computer Science and Engineering /4.00 GPA 09/2022 - 06/2027 Relevant courses: AI (graph search optimization), Deep learning (TensorFlow / PyTorch, NumPy, Pandas, Matplotlib), Computer Architecture (RISC-V, Assembly), Compiler Design (PLY, Regex, C++), Advanced Topics in NLP (HuggingFace, Transformers)\nPomona College Claremont, CA B.A. Computer Science & Mathematics /4.00 GPA 09/2018 - 05/2022 Thesis: “Fast Matrix Multiplication: A Song of Mathematics and Computer Science”"
  },
  {
    "objectID": "papers/investigating-cnns/index.html",
    "href": "papers/investigating-cnns/index.html",
    "title": "Investigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation",
    "section": "",
    "text": "Abstract\nWhile training robots to navigate autonomously is a growing area of research, collecting large and diverse datasets from real environments is expensive and time-consuming. We seek to alleviate this problem by creating an autonomous agent trained inside a simulation created with Unreal Engine. We first create a dataset of over 10,000 images taken from within the simulation. Then, we use this data to train a model using different architectures to determine which one performs the best inside of the simulation. Finally, we evaluate the model’s performance in a different simulation to determine if the learning can be generalized. We found that the model trained on our data set could make it through a considerable amount of the mazes simulated in Unreal Engine, but failed to find similar success in other simulated environments.\n\n\nIntroduction\nWith improving technology, graphics processing units (GPUs) are utilized more in computer graphics. One field that has reaped the benefits of GPU improvement is simulations. Physics simulations that required data parallelism and intensive computation are now more accessible than ever. One application of simulation is in autonomous navigation. The high cost of building and maintaining a robot that can autonomously traverse terrain makes it challenging to implement computational intelligence. One way to test artificial intelligence in robots is through simulations. We are interested in examining neural network performance across different simulations and real life. In particular, we are looking to see how an agent can traverse a maze using a neural network (NN). We want to observe a robot learn to navigate a terrain, identifying a path to reach a goal.\nNNs are useful in that we can feed in images to a model and have it return a direction for which a robot can interpret and move accordingly. For this cross-simulation experiment, we have selected a handful of architectures that were trained in a Raycast simulation. We believe they will cross the reality gap best in the Unreal simulation. Such architectures include ResNet, AlexNet, and an original architecture that takes an image and concatenates the previous command string. The Unreal simulation serves as a proxy for how well the models might perform in real life. First, we train models in the Unreal simulation and see how they perfom in the environment they were trained in. After, we train models in the Unreal simulation and observe their performance in the raycasting simulation.\n\nRead our full paper here!"
  },
  {
    "objectID": "papers/ceriad/index.html",
    "href": "papers/ceriad/index.html",
    "title": "Investigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation",
    "section": "",
    "text": "Abstract\nDeep reinforcement learning (DRL) is becoming an increasingly common technique to train agents to accomplish autonomous driving tasks (Kiran et al., 2021). However, DRL models, trained end-to-end, lack internal reasoning and planning for complex states and large action spaces (Dasgupta et al., 2023). Large language models (LLMs) could provide reasoning for autonomous vehicle systems and have demonstrate high-level reasoning across various tasks such as text generation (Wei et al., 2023), visual question and answer (VQA) (Liu et al., 2023a), and image generation (Huang et al., 2022). We investigate how to integrate cognitive reasoning from LLMs into autonomous vehicles that are trained with DRL. In this paper, we adapt the Planner-Actor-Reporter framework for autonomous driving tasks in Highway-Env and CARLA (Dasgupta et al., 2023; Leurent, 2018; Dosovitskiy et al., 2017). The work from Dasgupta et al. (2023) introduce the Planner-Actor-Reporter framework and apply it in gridworld reinforcement learning environments. We extend on their research by applying their framework to two autonomous driving environments, leveraging LLaVA as a visual reporter, and demonstrating common-sense driving reasoning using GPT-3.5-Turbo as the planner (Liu et al., 2023a). This paper opens new possibilities for safe, reliable, and interpretable decision making for autonomous vehicles by leveraging LLMs.\n\nCheckout our full paper here!"
  }
]